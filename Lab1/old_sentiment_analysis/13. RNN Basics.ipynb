{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Data for Training RNNs in Keras\n",
    "When we use recurrent networks, the most common form of data is text. In order to represent a sentence, we will need to make a few decisions. \n",
    "- Will our network fundamentally represent characters? Words? What are the building blocks for our structure?\n",
    "- What is the maximum sequence length? \n",
    "- What is my maximum vocabulary size?\n",
    "- What style of RNN should I use?\n",
    "- Can I incorporate an Embedding? What size?\n",
    "- What is my problem type? Many to one? Many to many? Sequence-to-Sequence?\n",
    "\n",
    "A great tutorial that this notebook also uses as base code:\n",
    "- http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "Let's start with the example above and see what the representation in Keras should look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 1000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences to be the same length\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (25000, 500)\n",
      "<class 'numpy.ndarray'> (500,)\n",
      "Vocabulary size: 999\n",
      "(25000,) 0 1\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train), X_train.shape)\n",
    "print(type(X_train[0]), X_train[0].shape)\n",
    "print('Vocabulary size:', np.max(X_train))\n",
    "print(y_train.shape, np.min(y_train), np.max(y_train))\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB ratings data contains: a number of different reviews and the sentiment of the review as positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example without the FOR loop\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, SimpleRNN\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "EMBED_SIZE = 50\n",
    "RNN_STATESIZE = 100\n",
    "rnns = []\n",
    "input_holder = Input(shape=(X_train.shape[1], ))\n",
    "shared_embed = Embedding(top_words, # input dimension (max int of OHE)\n",
    "                EMBED_SIZE, # output dimension size\n",
    "                input_length=max_review_length)(input_holder) # number of words in each sequence\n",
    "\n",
    "\n",
    "x = SimpleRNN(RNN_STATESIZE, dropout=0.2, recurrent_dropout=0.2)(shared_embed)\n",
    "x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "simple_rnn_model = Model(inputs=input_holder,outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 50)           50000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100)               15100     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,201\n",
      "Trainable params: 65,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 50)           50000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               60400     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,501\n",
      "Trainable params: 110,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 50)           50000     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100)               45600     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 95,701\n",
      "Trainable params: 95,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# create LSTM\n",
    "x = LSTM(RNN_STATESIZE, dropout=0.2, recurrent_dropout=0.2)(shared_embed)\n",
    "x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "lstm_model = Model(inputs=input_holder,outputs=x)\n",
    "\n",
    "# create GRU\n",
    "x = GRU(RNN_STATESIZE, dropout=0.2, recurrent_dropout=0.2)(shared_embed)\n",
    "x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "gru_model = Model(inputs=input_holder,outputs=x)\n",
    "\n",
    "# lr_schedule = ExponentialDecay(\n",
    "#     initial_learning_rate=0.1,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=0.95) \n",
    "\n",
    "opt = Adam(lr=0.0001, epsilon=0.0001, clipnorm=1.0)\n",
    "\n",
    "simple_rnn_model.compile(loss='binary_crossentropy', \n",
    "              optimizer= opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy', \n",
    "              optimizer= opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "gru_model.compile(loss='binary_crossentropy', \n",
    "              optimizer= opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(simple_rnn_model.summary())\n",
    "print(lstm_model.summary())\n",
    "print(gru_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    lstm_model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 87s 220ms/step - loss: 0.7098 - accuracy: 0.5037 - val_loss: 0.6953 - val_accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 84s 216ms/step - loss: 0.6936 - accuracy: 0.5208 - val_loss: 0.6877 - val_accuracy: 0.5456\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 86s 220ms/step - loss: 0.6871 - accuracy: 0.5353 - val_loss: 0.6786 - val_accuracy: 0.5354\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1054, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 1247, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 232, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable lstm/lstm_cell/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.Adam.'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# takes about 2 hours to run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m simple_rnn_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m gru_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileixfo8m7z.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1054, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 1247, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer.py\", line 232, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable lstm/lstm_cell/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.Adam.'\n"
     ]
    }
   ],
   "source": [
    "# takes about 2 hours to run\n",
    "simple_rnn_model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n",
    "lstm_model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n",
    "gru_model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 2 hours to run\n",
    "simple_rnn_model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n",
    "lstm_model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n",
    "gru_model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this operation can take time \n",
    "yhat_rnn = simple_rnn_model.predict(X_test)\n",
    "yhat_lstm = lstm_model.predict(X_test)\n",
    "yhat_gru = gru_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = [mt.accuracy_score(y_test,np.round(yhat_rnn)),\n",
    "       mt.accuracy_score(y_test,np.round(yhat_lstm)),\n",
    "       mt.accuracy_score(y_test,np.round(yhat_gru)),\n",
    "      ]\n",
    "\n",
    "plt.bar([1,2,3],acc)\n",
    "plt.xticks([1,2,3],['Simple','LSTM','GRU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# RNNs with Pre-processing in Keras\n",
    "\n",
    "## Moving to 20 Newsgroups\n",
    "So we should be able to convert a new dataset into the same format as above. Let's do this from scratch, converting the 20 news groups data. \n",
    "- http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "We looked at this a while back when we created the tf-idf and bag-of-words models. This time, we are not going to get rid of the sequence of words for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert 20 newsgroups into keras sequence format\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "bunch = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# display a random document and label\n",
    "idx = round(np.random.rand()*len(bunch.data))\n",
    "print('--------Random Document---------')\n",
    "print('================================')\n",
    "print('Document Label: ',bunch.target_names[bunch.target[idx]])\n",
    "print('================================')\n",
    "print(\"\\n\".join(bunch.data[idx].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_ART_LEN = 1000 # maximum and minimum number of words\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(bunch.data)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(bunch.data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(bunch.target)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it! The representation is now:\n",
    "- each word is converted to an integer \n",
    "- each article is a series of integers that represent the correct ordering of words\n",
    "- the target is one hot encoded\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=bunch.target, \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "# print some stats of the data\n",
    "print(\"X_train Shape:\",X_train.shape, \"Label Shape:\", y_train_ohe.shape)\n",
    "uniq_classes = np.sum(y_train_ohe,axis=0)\n",
    "plt.bar(list(range(20)),uniq_classes)\n",
    "plt.xticks(list(range(20)), bunch.target_names, rotation='vertical')\n",
    "plt.ylabel(\"Number of Instances in Each Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the embedding\n",
    "But this is going to be a more involved process. Maybe we can speed up the training by loading up a pre-trained embedding of the words?! Recall that the GloVe embedding uses pre-trained word embeddings in order to group contextually similar words together. \n",
    "\n",
    "A number of versions of the GloVe embedding matrix are pre-trained using various document. Let's use a version trained from 6B different documents from wikipedia (this is actually one of the smaller datasets trained upon--so the vocabulary might be a bit limited).\n",
    "\n",
    "Let's use the GloVe word embedding in keras. We will follow the example at:\n",
    "- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "You can download the nearly 1GB pretrained embeddings here:\n",
    "- http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Let's take a quick look at the format of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -a \"./Glove/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head \"./Glove/glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('../data/embeddings/glove/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
    "      \"Total words found:\",found_words, \"\\n\",\n",
    "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# save this embedding now\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(embedding_layer)\n",
    "rnn.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "print(rnn.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "tmp = rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=6, batch_size=64)\n",
    "history.append( tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extend the training by a number of epochs\n",
    "tmp = rnn.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=6, batch_size=64)\n",
    "history.append( tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['accuracy','val_accuracy','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in history])\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['accuracy'])\n",
    "plt.plot(combined['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(combined['loss'])\n",
    "plt.plot(combined['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these converged? No! Not even close!!! We need to run these for many more epochs. \n",
    "\n",
    "## [Back to Slides]\n",
    "\n",
    "# From Recurrent back to Convolutional\n",
    "Since all the sequences are now the same length, it is possible to construct a convolutional network on top of the embedded sequence outputs. Once embedded, there really is not too much difference in representation about the data and an image (though the data clearly has a different intuition). We will follow these steps:\n",
    "- use an embedding layer to translate to a dense representation of the entire sequence\n",
    "- train 1D filters to convolve with the output of the embedded sequence\n",
    " - note: 1D filters doesn't mean they are only one dimension, it means the filters are looped through in a single dimension. The size of the filter is [`kernel_size x embed_size`]. It loops through the entire embedding, over a number of words at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but wait, if we are just making the sequences of this all the same size\n",
    "#  can't we just use a 1-D convolutional network? \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "EMBED_SIZE = 100 \n",
    "sequence_input = Input(shape=(MAX_ART_LEN,), dtype='int32')\n",
    "# starting size: 1000\n",
    "embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "# after conv, size becomes: 1000-4=996\n",
    "x = Dropout(0.05)(x)\n",
    "x = MaxPooling1D(5)(x)# after max pool, 996/5 = 199\n",
    "x = Dropout(0.15)(x)\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# new size is 195\n",
    "x = MaxPooling1D(5)(x) # after max pool, size is 195/5 = 39\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# after convolution, size becomes 35 elements long\n",
    "x = MaxPooling1D(35)(x) # this is the size to globally flatten \n",
    "# flattened vector max pools across each of the 35 elements\n",
    "# so vectors is now 128 dimensions (same as num output filters)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(128, activation='relu',\n",
    "          kernel_initializer='he_uniform')(x)\n",
    "\n",
    "preds = Dense(NUM_CLASSES, activation='softmax',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "histories = []\n",
    "tmp = model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=2, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=10, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['acc','val_acc','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in histories])\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['acc'])\n",
    "plt.plot(combined['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(combined['loss'])\n",
    "plt.plot(combined['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. So we seem to be maxing out the performance on the validation set. The results are pretty good--I would expect these to be about a top 10 submission on a class-based Kaggle competition. Nice!\n",
    "- https://inclass.kaggle.com/c/cs5740-20-newsgroups-classification/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[End of Lecture Demo]**\n",
    "\n",
    "Below is an archive of discussions from previous semesters. While it is interesting, it is not critical to the understanding of recurrent neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============[ARCHIVED]=============\n",
    "# But others have done much better on 20 News groups! \n",
    "\n",
    "Why are we not getting the 95% on the data? Let's take a look at the original posting by the Keras blog. They claim that they get validation accuracy of 95% after two epochs!\n",
    "\n",
    "Why are we not getting that same accuracy?  \n",
    "- https://github.com/kimardenmiller/NLP_CNN/blob/master/Embeddings/0.0.4%20word%20index%20save.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = 'large_data/glove'\n",
    "TEXT_DATA_DIR = 'large_data/20news-bydate/20news-bydate-train/'\n",
    "TEXT_DATA_DIR = 'large_data/20_newsgroups/'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# first, build index mapping words in the embeddings set to their embedding vector\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open('./Glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text data set')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label news_group to numeric id\n",
    "labels = []  # list of label ids\n",
    "for news_group in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, news_group)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[news_group] = label_id\n",
    "        for post in sorted(os.listdir(path)):\n",
    "            if post.isdigit():\n",
    "                post_path = os.path.join(path, post)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(post_path)\n",
    "                else:\n",
    "                    f = open(post_path, encoding='latin-1')\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "print(texts[0])\n",
    "\n",
    "# override with sklearn\n",
    "# texts = bunch.data\n",
    "# labels = bunch.target\n",
    "# labels_index = bunch.target_names\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "pickle.dump(tokenizer, open('large_data/tokenizer.004.p', 'wb'))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# print('Word index', word_index.values())\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Sample data tensor:', data[0, 0:])\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Sample label tensor:', labels[690:750, 0:])\n",
    "# sys.exit()\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(nb_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=2, batch_size=128)\n",
    "\n",
    "#  Dec 30, 2016  loss: 0.3069 - acc: 0.8908 - val_loss: 0.1421 - val_acc: 0.9549\n",
    "\n",
    "# # First evaluation of the model\n",
    "# scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "# print(\"Accuracy of Run Model: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# model.save('../saved_models/0.0.4_model.h5')\n",
    "# np.save('../saved_models/0.0.4_word_index.npy', word_index)\n",
    "\n",
    "# del model  # deletes the existing model\n",
    "\n",
    "# # returns a compiled model identical to the previous one\n",
    "# model = load_model('../saved_models/0.0.4_model.h5')\n",
    "\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "# print(\"Accuracy of Disk-Loaded Model: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
