{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Data for Training Sequential Networks in Keras\n",
    "When we use sequential networks, the most common form of data is text. In order to represent a sentence, we will need to make a few decisions. \n",
    "- Will our network fundamentally represent characters? Words? What are the building blocks for our structure?\n",
    "- What is the maximum sequence length? \n",
    "- What is my maximum vocabulary size?\n",
    "- What style of sequence network should I use?\n",
    "- Can I incorporate an Embedding? What size?\n",
    "- What is my problem type? Many to one? Many to many? Sequence-to-Sequence?\n",
    "\n",
    "Let's start with a simple example from the IMdB dataset and see what the representation in Keras should look like for a sequential convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 1000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences to be the same length\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train), X_train.shape)\n",
    "print(type(X_train[0]), X_train[0].shape)\n",
    "print('Vocabulary size:', np.max(X_train))\n",
    "print(y_train.shape, np.min(y_train), np.max(y_train))\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB ratings data contains: a number of different reviews and the sentiment of the review as positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
    "from tensorflow.keras.layers import Subtract\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "EMBED_SIZE = 50\n",
    "# the input is a list of integers, 500 long\n",
    "sequence_input = Input(shape=(X_train.shape[1], ))\n",
    "\n",
    "# this will reduce the input dimension from VOCAB_SIZE to 50 for each word\n",
    "# the lenght will be the maximum number of words in a document, so 500\n",
    "embedded_sequences = Embedding(top_words, # input dimension (max int of OHE)\n",
    "                EMBED_SIZE, # output dimension size\n",
    "                input_length=max_review_length)(sequence_input) # number of words in each sequence\n",
    "\n",
    "# starting sequence size is 500 (words) by 50 (embedded features)\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "# after conv, length becomes: 500-4=496 \n",
    "# so overall size is 496 by 64\n",
    "\n",
    "# now pool across time\n",
    "x = MaxPooling1D(5)(x)# after max pool, 496/5 -> 99 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# extract additional features\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# new size is 95 after the conovlutions\n",
    "x = MaxPooling1D(5)(x) # after max pool, size is 95/5 = 19 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# more features through CNN processing!\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# after convolution, size becomes 15 elements long\n",
    "# Take the mean of these elements across features, result is 64 elements\n",
    "x_mean = GlobalAveragePooling1D()(x) # this is the size to globally flatten \n",
    "\n",
    "# Take the variance of these elements across features, result is 64 elements\n",
    "x_tmp = Subtract()([x,x_mean])\n",
    "x_std = GlobalAveragePooling1D()(x_tmp**2)\n",
    "\n",
    "x = Concatenate(name='concat_1')([x_mean,x_std])\n",
    "\n",
    "x = Dense(64, activation='relu',\n",
    "          kernel_initializer='he_uniform')(x)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "preds = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# you will need to install pydot properly on your machine to get this running\n",
    "plot_model(\n",
    "    model, to_file='model.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='LR', expand_nested=False, dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import \n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True) \n",
    "# clipnorm=1.0, \n",
    "\n",
    "opt = Adam(epsilon=0.0001, learning_rate=lr_schedule)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "history = []\n",
    "tmp = model.fit(X_train, y_train, epochs=5, \n",
    "                batch_size=64, \n",
    "                validation_data=(X_test, y_test))\n",
    "history.append( tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this as many times as we want! Adds 5 epochs each time\n",
    "tmp = model.fit(X_train, y_train, epochs=5, \n",
    "                batch_size=64, \n",
    "                validation_data=(X_test, y_test))\n",
    "history.append( tmp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['acc','val_acc','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in history])\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['acc'])\n",
    "plt.plot(combined['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(combined['loss'])\n",
    "plt.plot(combined['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[back to slides]**\n",
    "\n",
    "# Transformers\n",
    "TODO: explain more on transformers. Covered in lecture, but no description ready yet here. **This is a major reason that this is still classified as experimental.** \n",
    "\n",
    "Example manipulated from the following: https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n",
    "Also, the reference for using multi heads is here:\n",
    "https://keras.io/api/layers/attention_layers/multi_head_attention/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# The transformer architecture \n",
    "class TransformerBlock(Layer): # inherit from Keras Layer\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        # setup the model heads and feedforward network\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, \n",
    "                                      key_dim=embed_dim)\n",
    "        \n",
    "        # make a two layer network that processes the attention\n",
    "        self.ffn = Sequential()\n",
    "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
    "        self.ffn.add( Dense(embed_dim) )\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # apply the layers as needed (similar to PyTorch)\n",
    "        \n",
    "        # get the attention output from multi heads\n",
    "        # Using same inpout here is self-attention\n",
    "        # call inputs are (query, value, key) \n",
    "        # if only two inputs given, value and key are assumed the same\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        \n",
    "        # create residual output, with attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out1 = self.dropout1(out1, training=training)\n",
    "        \n",
    "        # place through feed forward after layer norm\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        # apply dropout if training\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        #return the residual from Dense layer\n",
    "        return out2\n",
    "    \n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # create two embeddings \n",
    "        # one for processing the tokens (words)\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, \n",
    "                                   output_dim=embed_dim)\n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, \n",
    "                                 output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = TokenAndPositionEmbedding(X_train.shape[1], top_words, embed_dim)(inputs)\n",
    "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model_xformer = Model(inputs=inputs, outputs=outputs)\n",
    "print(model_xformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xformer.compile(optimizer='adam', \n",
    "                      loss='binary_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "history = model_xformer.fit(\n",
    "    X_train, y_train, batch_size=64, epochs=2, \n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yhat_cnn = model.predict(X_test)\n",
    "yhat_xformer = model_xformer.predict(X_test)\n",
    "\n",
    "acc = [mt.accuracy_score(y_test,np.round(yhat_cnn)),\n",
    "       mt.accuracy_score(y_test,np.round(yhat_xformer)),\n",
    "      ]\n",
    "\n",
    "plt.bar([1,2],acc)\n",
    "plt.xticks([1,2],['CNN','XFORMER'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Sequences with Pre-processing in Keras\n",
    "\n",
    "## Moving to 20 Newsgroups\n",
    "So we should be able to convert a new dataset into the same format as above. Let's do this from scratch, converting the 20 news groups data. \n",
    "- http://qwone.com/~jason/20Newsgroups/\n",
    "\n",
    "We looked at this a while back when we created the tf-idf and bag-of-words models. This time, we are not going to get rid of the sequence of words for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 20 newsgroups into keras sequence format\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "bunch = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bunch.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# display a random document and label\n",
    "idx = round(np.random.rand()*len(bunch.data))\n",
    "print('--------Random Document---------')\n",
    "print('================================')\n",
    "print('Document Label: ',bunch.target_names[bunch.target[idx]])\n",
    "print('================================')\n",
    "print(\"\\n\".join(bunch.data[idx].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_ART_LEN = 1000 # maximum and minimum number of words\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(bunch.data)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(bunch.data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(bunch.target)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it! The representation is now:\n",
    "- each word is converted to an integer \n",
    "- each article is a series of integers that represent the correct ordering of words\n",
    "- the target is one hot encoded\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=bunch.target, \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "# print some stats of the data\n",
    "print(\"X_train Shape:\",X_train.shape, \"Label Shape:\", y_train_ohe.shape)\n",
    "uniq_classes = np.sum(y_train_ohe,axis=0)\n",
    "plt.bar(list(range(20)),uniq_classes)\n",
    "plt.xticks(list(range(20)), bunch.target_names, rotation='vertical')\n",
    "plt.ylabel(\"Number of Instances in Each Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the embedding\n",
    "But this is going to be a more involved process. Maybe we can speed up the training by loading up a pre-trained embedding of the words?! Recall that the GloVe embedding uses pre-trained word embeddings in order to group contextually similar words together. \n",
    "\n",
    "A number of versions of the GloVe embedding matrix are pre-trained using various document. Let's use a version trained from 6B different documents from wikipedia (this is actually one of the smaller datasets trained upon--so the vocabulary might be a bit limited).\n",
    "\n",
    "Let's use the GloVe word embedding in keras. We will follow the example at:\n",
    "- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "You can download the nearly 1GB pretrained embeddings here:\n",
    "- http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Let's take a quick look at the format of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -a \"./Glove/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head \"./Glove/glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('./Glove/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
    "      \"Total words found:\",found_words, \"\\n\",\n",
    "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# save this embedding now\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# Start with Convolutional (no std vectors)\n",
    "Since all the sequences are now the same length, it is possible to construct a convolutional network on top of the embedded sequence outputs. Once embedded, there really is not too much difference in representation about the data and an image (though the data clearly has a different intuition). We will follow these steps:\n",
    "- use an embedding layer to translate to a dense representation of the entire sequence\n",
    "- train 1D filters to convolve with the output of the embedded sequence\n",
    " - note: 1D filters doesn't mean they are only one dimension, it means the filters are looped through in a single dimension. The size of the filter is [`kernel_size x embed_size`]. It loops through the entire embedding, over a number of words at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "EMBED_SIZE = 100  # same size as loaded from GLOVE\n",
    "sequence_input = Input(shape=(MAX_ART_LEN,), dtype='int32')\n",
    "# starting size: 1000\n",
    "embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "# after conv, size becomes: 1000-4=996\n",
    "x = MaxPooling1D(5)(x)# after max pool, 996/5 = 199\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# new size is 195\n",
    "x = MaxPooling1D(5)(x) # after max pool, size is 195/5 = 39\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# after convolution, size becomes 35 elements long\n",
    "x = MaxPooling1D(35)(x) # this is the size to globally flatten \n",
    "# flattened vector max pools across each of the 35 elements\n",
    "# so vectors is now 128 dimensions (same as num output filters)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu',\n",
    "          kernel_initializer='he_uniform')(x)\n",
    "\n",
    "preds = Dense(NUM_CLASSES, activation='softmax',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "# if representing as OHE, use categorical_crossentropy\n",
    "# if representing the class as an integer, use sparse_categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "histories = []\n",
    "tmp = model.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=2, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.fit(X_train, y_train_ohe, \n",
    "                validation_data=(X_test, y_test_ohe),\n",
    "                epochs=20, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.fit(X_train, y_train_ohe, \n",
    "                validation_data=(X_test, y_test_ohe),\n",
    "               epochs=20, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['acc','val_acc','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in histories])\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['acc'])\n",
    "plt.plot(combined['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(combined['loss'])\n",
    "plt.plot(combined['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. So we seem to be maxing out the performance on the validation set. The results are pretty good--I would expect these to be about a top 10 submission on a class-based Kaggle competition. Nice!\n",
    "- https://inclass.kaggle.com/c/cs5740-20-newsgroups-classification/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Convolutional with Variance Pooling\n",
    "Add varinace pooling results on 20 news groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "EMBED_SIZE = 100  # same size as loaded from GLOVE\n",
    "sequence_input = Input(shape=(MAX_ART_LEN,), dtype='int32')\n",
    "# starting size: 1000\n",
    "embedded_sequences = embedding_layer(sequence_input) # from previous embedding\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "# after conv, size becomes: 1000-4=996\n",
    "x = MaxPooling1D(5)(x)# after max pool, 996/5 = 199\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# new size is 195\n",
    "x = MaxPooling1D(5)(x) # after max pool, size is 195/5 = 39\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# after convolution, size becomes 15 elements long\n",
    "# Take the mean of these elements across features, result is 128 elements\n",
    "x_mean = GlobalAveragePooling1D()(x) # this is the size to globally flatten \n",
    "\n",
    "# Take the variance of these elements across features, result is 128 elements\n",
    "x_tmp = Subtract()([x,x_mean])\n",
    "x_std = GlobalAveragePooling1D()(x_tmp**2)\n",
    "\n",
    "x = Concatenate(name='concat_1')([x_mean,x_std])\n",
    "\n",
    "\n",
    "x = Dense(128, activation='relu',\n",
    "          kernel_initializer='he_uniform')(x)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "preds = Dense(NUM_CLASSES, activation='softmax',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model_xvec = Model(sequence_input, preds)\n",
    "\n",
    "# if representing as OHE, use categorical_crossentropy\n",
    "# if representing the class as an integer, use sparse_categorical_crossentropy\n",
    "model_xvec.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model_xvec.summary())\n",
    "\n",
    "histories = []\n",
    "tmp = model_xvec.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe),\n",
    "          epochs=2, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model_xvec.fit(X_train, y_train_ohe, \n",
    "                validation_data=(X_test, y_test_ohe),\n",
    "               epochs=20, batch_size=128)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['acc','val_acc','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in histories])\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['acc'])\n",
    "plt.plot(combined['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(combined['loss'])\n",
    "plt.plot(combined['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Transformer, 20 News Groups\n",
    "Now lets run the same experiment with a transformer architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's replace the original embedding in the xformer\n",
    "# with our custom GloVe embedding\n",
    "\n",
    "class GloveTokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # create two embeddings \n",
    "        # one for processing the tokens, pretrained (words)\n",
    "        self.token_emb = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)\n",
    "        \n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(MAX_ART_LEN, \n",
    "                                 EMBED_SIZE, \n",
    "                                 input_length=MAX_ART_LEN,\n",
    "                                 trainable=True\n",
    "                                )\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 (for IMdB) by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = GloveTokenAndPositionEmbedding()(inputs)\n",
    "x = TransformerBlock(EMBED_SIZE, num_heads, ff_dim)(x)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model_xformer20 = Model(inputs=inputs, outputs=preds)\n",
    "print(model_xformer20.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xformer20.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "histories = []\n",
    "tmp = model_xformer20.fit(X_train, y_train_ohe, \n",
    "                validation_data=(X_test, y_test_ohe),\n",
    "               epochs=6, batch_size=16)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "tmp = model_xformer20.fit(X_train, y_train_ohe, \n",
    "                validation_data=(X_test, y_test_ohe),\n",
    "               epochs=8, batch_size=16)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['acc','val_acc','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in histories])\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['acc'])\n",
    "plt.plot(combined['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(combined['loss'])\n",
    "plt.plot(combined['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Transformer More Layers!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = GloveTokenAndPositionEmbedding()(inputs)\n",
    "x = TransformerBlock(EMBED_SIZE, num_heads, ff_dim)(x)\n",
    "x = TransformerBlock(EMBED_SIZE, num_heads, ff_dim)(x)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(NUM_CLASSES, activation='softmax',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "model_xformer20 = Model(inputs=inputs, outputs=preds)\n",
    "print(model_xformer20.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xformer20.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "histories = []\n",
    "tmp = model_xformer20.fit(X_train, y_train_ohe, \n",
    "                validation_data=(X_test, y_test_ohe),\n",
    "               epochs=6, batch_size=16)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "tmp = model_xformer20.fit(X_train, y_train_ohe, \n",
    "                validation_data=(X_test, y_test_ohe),\n",
    "               epochs=8, batch_size=16)\n",
    "histories.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# combine all the history from training together\n",
    "combined = dict()\n",
    "for key in ['acc','val_acc','loss','val_loss']:\n",
    "    combined[key] = np.hstack([x.history[key] for x in histories])\n",
    "    \n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(combined['acc'])\n",
    "plt.plot(combined['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(122)\n",
    "plt.plot(combined['loss'])\n",
    "plt.plot(combined['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
